{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AI for Physical Infrastructure","text":"<p>\"Uber didn't build the roads.\"</p>"},{"location":"#1-strategic-vision","title":"1. Strategic Vision","text":""},{"location":"#the-infrastructure-analogy","title":"The Infrastructure Analogy","text":"<p>Uber didn't build the roads, bridges, or tunnels. They built a service layer on top of existing public infrastructure to revolutionize transportation. </p> <p>Similarly, our value proposition is AI for Infrastructure (Telecom, Energy, Utilities). We do not build the \"AI roads\"\u2014the foundation models, GPU clusters, and training pipelines. We build the verification and application layer that allows physical infrastructure companies to safely use these powerful new tools.</p>"},{"location":"#why-we-build-on-the-giants","title":"Why We Build On the Giants","text":"<p>Building foundation models is a capital-intensive \"race to the bottom.\" Instead, we leverage Google Cloud Vertex AI to treat intelligence as a reliable, scalable utility.</p>"},{"location":"#1-no-vendor-or-model-lock-in","title":"1. No Vendor or Model Lock-in","text":"<p>The AI landscape shifts weekly. By building on the Vertex Model Garden, we completely decouple our business logic from specific model providers. *   Gemini Models: Utilized for massive context windows and native multimodal (text/image/video) reasoning. *   Claude (Anthropic): Available as a first-class citizen for high-nuance tasks. *   Open Models (Llama, Mistral): Deployable for cost-sensitive, high-volume, or data-sovereign workloads.</p> <p>We can swap the engine, but the vehicle\u2014our Claims Platform\u2014remains the same.</p>"},{"location":"#2-economic-efficiency","title":"2. Economic Efficiency","text":"<p>We operate on a purely OPEX, pay-as-you-go model (see Vertex AI Pricing). *   Zero CapEx: We do not purchase H100s, manage data center cooling, or depreciate hardware. *   Dynamic Optimization: We route simple extraction tasks to cost-effective models (e.g., Gemini Flash) and complex reasoning to \"Pro\" or \"Ultra\" variants. This allows us to protect margins while scaling from 10 to 10 million documents without architectural changes.</p>"},{"location":"#2-technical-execution-the-business-of-attesting-truth","title":"2. Technical Execution: The Business of Attesting Truth","text":""},{"location":"#vision-attestation-canonical-models","title":"Vision: Attestation &amp; Canonical Models","text":"<p>The core business goal is to shift from ephemeral data extraction to a system that enables both structured attestation and interactive document engagement. Customers and internal users want to be able to chat with documents to ask questions and refine them at the same time for feedback loops - this functionality serves practical business needs, not just convenience. The system also moves away from rigid, hard-coded automation pipelines where random pieces of data are extracted and manually assembled to build CAD models. Current approaches follow very specific, rigid patterns that require extensive custom code for each document type, creating maintenance burdens and limiting scalability. Our AI attestation approach allows us to scale both in domain knowledge and code-wise. We propose a system where every piece of data is a signed, evidence-backed Claim that can be verified and trusted.</p>"},{"location":"#why-this-scales","title":"Why This Scales","text":"<ul> <li>Decoupling Extraction from Knowledge: By building a canonical site model through fused claims, the system handles N-customers and N-document types without constant code intervention.</li> <li>Verifiable Facts: As more data is processed through the system, confidence scores increase, creating a self-reinforcing cycle of accuracy and reliability that the business can rely on for high-stakes decisions like engineering and construction.</li> </ul>"},{"location":"#3-architecture-components","title":"3. Architecture &amp; Components","text":"<p>The architecture follows a Supervisor/Worker pattern, utilizing Domain-Driven Design (DDD) to isolate business rules from technical orchestration.</p>"},{"location":"#31-component-breakdown","title":"3.1 Component Breakdown","text":"<ul> <li>The Supervisor (Planner): Acts as a \"planner over missing claims.\" It identifies what evidence is required to reach a target schema version and delegates tasks dynamically.</li> <li>Domain Workers: Specialized agents (e.g., RFDS Agent, Lease Agent) that are stateless and focused on generating specific claims.</li> <li>Stateless by Design: We explicitly reject the \"Stateful Graph\" anti-pattern (like complex LangGraph chains). State is heavy, brittle, and difficult to debug. Instead, we treat state as a natural byproduct of the ecosystem:<ul> <li>Context: The immediate working set of data (e.g., the current document).</li> <li>Session: The ephemeral interaction history (e.g., \"Why did you choose that value?\").</li> <li>Memory: The long-term persistent store of validated truths (The Database).</li> </ul> </li> </ul>"},{"location":"#32-the-context-assembly-anti-pattern","title":"3.2 The \"Context Assembly\" Anti-Pattern","text":"<p>Many AI systems fail because they rely on \"Context Assembly\"\u2014manually stitching together strings from various sources to feed an LLM. This is fragile and error-prone. *   Our Approach: We use native primitives. Vertex AI Context Caching allows us to \"freeze\" massive documents into the model's native memory, eliminating the need to constantly re-upload or manually parse text. This reduces latency and cost while improving accuracy.</p>"},{"location":"#33-inputoutput-examples","title":"3.3 Input/Output Examples","text":"<p>The system uses a standardized protocol to ensure agents remain composable and the system remains vendor-agnostic.</p> <p>Example Input (Task Configuration): <pre><code>{\n  \"site_id\": \"WSUTH0035137\",\n  \"attachments\": [\n    \"s3://inorsa-dev/agent-nora/ansco-data-validation-demo/WSUTH0035137/AE201.24-10-25.WSUTH0035137.IDL04374.pdf\",\n    \"s3://inorsa-dev/agent-nora/ansco-data-validation-demo/WSUTH0035137/DE113.24-10-25.WSUTH0035137.IDL04374.pdf\",\n    \"s3://inorsa-dev/agent-nora/ansco-data-validation-demo/WSUTH0035137/LS106 - WSUTH0035137.pdf\",\n    \"s3://inorsa-dev/agent-nora/ansco-data-validation-demo/WSUTH0035137/RFDS - WSUTH0035137 - 12781456.pdf\"\n  ]\n}\n</code></pre></p> <p>Example Output (Aggregated Claims - Comprehensive RFDS Structure): <pre><code>{\n  \"document_metadata\": {\n    \"title\": \"RF Design Study for Site WSUTH0035137\",\n    \"specification_id\": \"AE201.24-10-25.WSUTH0035137\",\n    \"revision\": \"A\",\n    \"authoring_organization\": \"Example Engineering Corp\",\n    \"publication_date\": \"2024-10-25\",\n    \"applicable_system\": \"Cellular Network Infrastructure\",\n    \"classification\": \"Internal Use\",\n    \"referenced_standards\": [\"3GPP TS 36.101\", \"FCC Part 24\", \"ETSI EN 301 908-1\"],\n    \"source_pages\": [1, 2, 3, 4, 5]\n  },\n\n  \"rf_system_overview\": {\n    \"system_type\": \"LTE/5G NR Base Station\",\n    \"application_domain\": \"Cellular Network Infrastructure\",\n    \"operating_modes\": [\"FDD\", \"TDD\"],\n    \"architecture_description\": \"MIMO 4x4 with Beamforming\",\n    \"frequency_bands\": [\n      {\n        \"band_name\": \"Band 13\",\n        \"lower_frequency\": {\n          \"value\": 746,\n          \"unit\": \"MHz\",\n          \"si_value_hz\": 746000000\n        },\n        \"upper_frequency\": {\n          \"value\": 756,\n          \"unit\": \"MHz\",\n          \"si_value_hz\": 756000000\n        },\n        \"confidence\": 0.95,\n        \"source_pages\": [4]\n      }\n    ]\n  },\n\n  \"performance_specifications\": {\n    \"frequency_characteristics\": {\n      \"channel_bandwidth\": [\n        {\n          \"value\": 20,\n          \"unit\": \"MHz\",\n          \"confidence\": 0.92,\n          \"source_pages\": [6]\n        }\n      ],\n      \"frequency_stability\": [\n        {\n          \"value\": 0.1,\n          \"unit\": \"ppm\",\n          \"confidence\": 0.88,\n          \"source_pages\": [7]\n        }\n      ]\n    },\n\n    \"power_and_gain\": {\n      \"output_power\": [\n        {\n          \"value\": 43,\n          \"unit\": \"dBm\",\n          \"confidence\": 0.94,\n          \"source_pages\": [8]\n        }\n      ],\n      \"antenna_gain\": [\n        {\n          \"value\": 18,\n          \"unit\": \"dBi\",\n          \"confidence\": 0.91,\n          \"source_pages\": [9]\n        }\n      ]\n    },\n\n    \"signal_quality\": {\n      \"adjacent_channel_rejection\": [\n        {\n          \"value\": 45,\n          \"unit\": \"dB\",\n          \"confidence\": 0.89,\n          \"source_pages\": [10]\n        }\n      ]\n    }\n  },\n\n  \"environmental_mechanical\": {\n    \"operating_temperature\": [\n      {\n        \"min_value\": -40,\n        \"max_value\": 65,\n        \"unit\": \"Celsius\",\n        \"confidence\": 0.96,\n        \"source_pages\": [12]\n      }\n    ],\n    \"physical_dimensions\": {\n      \"length\": {\n        \"value\": 600,\n        \"unit\": \"mm\",\n        \"confidence\": 0.90,\n        \"source_pages\": [13]\n      },\n      \"width\": {\n        \"value\": 400,\n        \"unit\": \"mm\",\n        \"confidence\": 0.90,\n        \"source_pages\": [13]\n      },\n      \"height\": {\n        \"value\": 200,\n        \"unit\": \"mm\",\n        \"confidence\": 0.90,\n        \"source_pages\": [13]\n      }\n    }\n  },\n\n  \"compliance_regulatory\": {\n    \"regulatory_standards\": [\"FCC Part 24\", \"IC RSS-129\", \"CE Marking\"],\n    \"certification_requirements\": [\"FCC ID\", \"ISED Certification\"],\n    \"safety_constraints\": [\"UL 62368-1\", \"IEC 62368-1\"]\n  },\n\n  \"tables\": [\n    {\n      \"table_title\": \"Power Amplifier Specifications\",\n      \"columns\": [\"Parameter\", \"Min\", \"Typ\", \"Max\", \"Unit\"],\n      \"rows\": [\n        [\"Output Power\", \"-\", \"40\", \"43\", \"dBm\"],\n        [\"Gain\", \"15\", \"18\", \"20\", \"dB\"]\n      ],\n      \"source_pages\": [8]\n    }\n  ],\n\n  \"extraction_metadata\": {\n    \"model_version\": \"gemini-3.0-pro\",\n    \"extraction_timestamp\": \"2024-11-05T10:30:00Z\",\n    \"confidence_aggregate\": 0.92\n  }\n}\n</code></pre></p> <p>This represents the final aggregated output after multiple individual claims have been collected and combined into a comprehensive document representation.</p>"},{"location":"#4-workflow-pattern-rough-spec-to-refined-output","title":"4. Workflow Pattern: Rough Spec to Refined Output","text":"<p>The system is designed to take a \"rough spec\" (raw document sets) and transform them into a \"refined output\" ready for technical drawings or engineering analysis.</p>"},{"location":"#41-looper-agents","title":"4.1 Looper Agents","text":"<p>Rather than retrying entire workflows when a task fails, we use Looper Agents for individual task refinement. *   Task Isolation: A single task is given to a worker. *   Internal Refinement: The worker loops internally\u2014reviewing its own work, checking against constraints, and refining the output until it meets the required confidence threshold. *   Efficiency: This prevents the \"cascading failure\" problem common in linear pipelines.</p>"},{"location":"#42-intermediary-checks","title":"4.2 Intermediary Checks","text":"<p>The architecture allows for specialized agents to intervene at any point in the lifecycle: *   Lease Agents: Validate claims against lease agreement constraints. *   Proposal Agents: Ensure the evolving site model aligns with the project proposal. *   Infrastructure Agents: Check for buildability against existing infrastructure standards.</p>"},{"location":"#43-no-vendor-lock-in-technical-layer","title":"4.3 No Vendor Lock-in (Technical Layer)","text":"<p>By defining truth through a Claims Protocol rather than model-specific prompts, the platform remains highly portable. We can swap underlying LLMs (e.g., from Gemini to GPT or local models) or OCR engines without rewriting the business logic, as long as they adhere to the input/output contract.</p>"},{"location":"#5-competitive-strategic-advantage","title":"5. Competitive &amp; Strategic Advantage","text":"<ul> <li>Platform Native: We build on the platform (Vertex AI) rather than against it. We avoid heavy middleware libraries that enforce their own state management paradigms.</li> <li>Stateless Scalability: By relying on native Cloud primitives (Cloud Run, Vertex AI, Pub/Sub), our agents scale to zero and handle massive concurrency without the \"State Management Tax.\"</li> <li>Deterministic Foundation: We use DocAI for structural extraction, providing a grounded foundation for the probabilistic LLM layers.</li> <li>Human-in-the-Loop (HITL): Discrepancies (e.g., SA vs. RFDS height conflicts) are handled via managed \"Pause and Resume\" workflows, treating human resolution as a high-confidence claim.</li> </ul>"},{"location":"#6-observability-trust-at-scale","title":"6. Observability: Trust at Scale","text":"<p>Observability in AI infrastructure is not just about system logs; it is about exposing the \"Chain of Thought\" and \"Chain of Custody\" for every decision. We provide tailored views for every stakeholder to ensure trust is verifiable, not assumed.</p>"},{"location":"#61-for-executives-roi-risk","title":"6.1 For Executives: ROI &amp; Risk","text":"<ul> <li>Cost-Per-Fact: Granular tracking of token consumption allows us to calculate the exact cost to extract a specific data point (e.g., \"It cost $0.03 to validate the antenna height\").</li> <li>Throughput Metrics: Dashboard views of site processing velocity\u2014shifting from \"sites per month\" to \"sites per hour.\"</li> <li>Risk Heatmaps: Identification of projects with low-confidence claims, allowing leadership to focus human QA resources where they matter most.</li> </ul>"},{"location":"#62-for-operations-the-control-plane","title":"6.2 For Operations: The Control Plane","text":"<ul> <li>Job Lifecycle Tracking: Real-time status of every site (Queued -&gt; Processing -&gt; Waiting for Human -&gt; Complete).</li> <li>Bottleneck Detection: Identification of specific agents (e.g., the Lease Validator) that are consistently stalling or requesting human help, signaling a need for process refinement.</li> </ul>"},{"location":"#63-for-domain-experts-the-glass-box-experience","title":"6.3 For Domain Experts: The \"Glass Box\" Experience","text":"<ul> <li>Deep Provenance: We never present a value without its source. Clicking a \"Claim\" (e.g., Antenna Azimuth) instantly opens the source PDF and draws a bounding box around the specific text or table cell used to derive that value.</li> <li>Audit Trails: A tamper-proof history of the \"Truth.\" We distinguish between values extracted by AI (with confidence scores) and values overridden or confirmed by a Human Engineer.<ul> <li>v1 (AI): 150.0 ft (80% confidence)</li> <li>v2 (Human): 155.0 ft (Reason: \"Updated per latest redlines\")</li> </ul> </li> </ul>"},{"location":"#64-for-developers-tracing-debugging","title":"6.4 For Developers: Tracing &amp; Debugging","text":"<ul> <li> <p>Full Stack Tracing: Leveraging Vertex AI Traces to see the full request/response lifecycle of every agent interaction.</p> </li> <li> <p>Eval-Driven Development: Continuous monitoring of agent performance against \"Golden Sets\" to ensure that model upgrades (e.g., to Gemini 3) actually improve accuracy before deployment.</p> </li> </ul>"},{"location":"architecture/","title":"Technical Architecture","text":""},{"location":"architecture/#agent-based-framework","title":"Agent-Based Framework","text":"<p>Our platform leverages Google's Agent Development Kit (ADK) with Vertex AI to implement a horizontally scalable, customer-agnostic system. The architecture follows a Supervisor/Worker pattern, utilizing Domain-Driven Design (DDD) to isolate business rules from technical orchestration.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ul> <li>The Supervisor (Planner): Acts as a \"planner over missing claims.\" It identifies what evidence is required to reach a target schema version and delegates tasks dynamically.</li> <li>Domain Workers: Specialized agents (e.g., RFDS Agent, Lease Agent) that are stateless and focused on generating specific claims.</li> <li>Stateless by Design: We explicitly reject the \"Stateful Graph\" anti-pattern (like complex LangGraph chains). State is heavy, brittle, and difficult to debug. Instead, we treat state as a natural byproduct of the ecosystem:<ul> <li>Context: The immediate working set of data (e.g., the current document).</li> <li>Session: The ephemeral interaction history (e.g., \"Why did you choose that value?\").</li> <li>Memory: The long-term persistent store of validated truths (The Database).</li> </ul> </li> </ul>"},{"location":"architecture/#document-processing-pipeline","title":"Document Processing Pipeline","text":"<p>Traditional approaches that rely on stateless libraries often result in LLMs being run repeatedly over entire documents, which is not optimal. Our approach optimizes document processing by:</p> <ol> <li>Structured Data Extraction First: Specialized parsers extract core data from documents (e.g., RFDS files) before LLM processing</li> <li>Targeted LLM Usage: LLMs receive only the relevant, extracted information rather than entire documents</li> <li>Efficient Resource Allocation: Reduces computational costs by minimizing unnecessary LLM calls</li> <li>Improved Accuracy: Structured parsing provides more reliable foundational data for LLM reasoning</li> </ol>"},{"location":"architecture/#agent-orchestration-patterns","title":"Agent Orchestration Patterns","text":"<p>The architecture supports multiple agent orchestration patterns that work together:</p>"},{"location":"architecture/#supervisorworker-pattern","title":"Supervisor/Worker Pattern","text":"<ul> <li>Central Coordination: The Supervisor Agent acts as the central coordinator that delegates tasks to specialized workers</li> <li>Specialized Workers: Worker Agents (e.g., RFDS Agent, Lease Agent) focus on specific extraction tasks</li> <li>State Management: The supervisor maintains overall workflow state while workers remain stateless</li> </ul>"},{"location":"architecture/#parallel-processing","title":"Parallel Processing","text":"<ul> <li>Concurrent Execution: Multiple agents can process different document types simultaneously</li> <li>Resource Optimization: Distributes workload efficiently across available resources</li> <li>Scalability: Enables processing of multiple documents in parallel</li> </ul>"},{"location":"architecture/#looper-agents","title":"Looper Agents","text":"<ul> <li>Iterative Refinement: Agents can refine their outputs through multiple iterations</li> <li>Quality Assurance: Continuous validation and improvement of results</li> <li>Self-Correction: Agents can identify and fix their own errors</li> </ul>"},{"location":"architecture/#infrastructure-management-with-adk","title":"Infrastructure Management with ADK","text":"<p>Solution with ADK: - Managed session state with ADK's built-in session management - Automatic conversation history tracking for each agent - Built-in state persistence and recovery - Native support for long-term memory without custom implementation</p>"},{"location":"architecture/#model-options-pricing","title":"Model Options &amp; Pricing","text":"<p>Our platform leverages the diverse model ecosystem available through Vertex AI, providing flexibility and cost optimization:</p>"},{"location":"architecture/#google-models","title":"Google Models","text":"<ul> <li>Gemini models pricing</li> <li>Advanced multimodal capabilities with massive context windows</li> <li>Optimized for complex reasoning and document analysis</li> </ul>"},{"location":"architecture/#partner-models","title":"Partner Models","text":"<ul> <li>Mistral models pricing</li> <li>Alternative reasoning capabilities for specialized tasks</li> <li>Competitive pricing for high-volume workloads</li> </ul>"},{"location":"architecture/#open-source-models","title":"Open Source Models","text":"<ul> <li>Qwen models pricing</li> <li>Cost-effective options for specific use cases</li> <li>Data sovereignty for sensitive workloads</li> </ul>"},{"location":"architecture/#memory-management-context-windows","title":"Memory Management &amp; Context Windows","text":""},{"location":"architecture/#advanced-memory-banks","title":"Advanced Memory Banks","text":"<p>Our platform utilizes Vertex AI Memory Banks for efficient long-term storage and retrieval of contextual information.</p>"},{"location":"architecture/#context-window-management","title":"Context Window Management","text":"<p>Unlike traditional approaches where adding RAG stores or message history can overload context windows, ADK implements sophisticated memory management:</p> <ul> <li>Selective Context Loading: Only relevant information is loaded into the active context window based on the current task</li> <li>Memory Bank Integration: Long-term memories are stored separately and retrieved on-demand rather than kept in the active context</li> <li>Session State Management: Conversation history is managed separately from the active context, preventing window overflow</li> <li>Dynamic Context Optimization: The system automatically determines which information is most relevant for the current decision, keeping the context window focused and efficient</li> <li>Context Compaction: ADK provides context compaction capabilities to automatically summarize and compress context when needed, ensuring optimal performance even with extensive document sets</li> </ul> <p>This approach ensures that adding more documents, conversation history, or RAG data doesn't simply overload the context window - instead, the system intelligently manages what information is actively available to the model at any given time.</p>"},{"location":"architecture/#claims-attestation-process","title":"Claims &amp; Attestation Process","text":"<p>The system implements a robust claims and attestation process that transforms raw extracted data into verifiable, trustworthy information:</p>"},{"location":"architecture/#tool-2-claim-builder","title":"Tool 2 \u2014 Claim Builder","text":"<p>Takes RF output \u2192 emits claim objects.</p> <p>Input Format: <pre><code>{\n  \"parameter\": \"Output Power\",\n  \"value\": 2,\n  \"unit\": \"dBm\",\n  \"limit_type\": \"max\",\n  \"source_pages\": [12],\n  \"table_id\": \"table_4\",\n  \"pipeline_versions\": {\n    \"parser\": \"2.1\",\n    \"extractor\": \"1.3\",\n    \"normalizer\": \"0.9\"\n  }\n}\n</code></pre></p> <p>Output Format (Claim Object): <pre><code>{\n  \"claim_id\": \"c_12345\",\n  \"key\": \"output_power_max\",\n  \"value\": 2,\n  \"unit\": \"dBm\",\n  \"confidence\": 0.98,\n  \"rationale\": \"Extracted from Power Amplifier Specifications table on page 12.\",\n  \"provenance\": {\n    \"document\": \"AE201.24-10-25.WSUTH0035137.IDL04374.pdf\",\n    \"page\": 12,\n    \"table_id\": \"table_4\",\n    \"bbox\": [100, 200, 300, 250]\n  },\n  \"validation_status\": \"verified\",\n  \"timestamp\": \"2024-11-05T10:30:00Z\"\n}\n</code></pre></p>"},{"location":"architecture/#tool-3-zk-attestation-simulator","title":"Tool 3 \u2014 ZK Attestation Simulator","text":"<p>Generates cryptographic proofs for claim verification:</p> <ul> <li>Commitment Hash: Cryptographic commitment to the claim</li> <li>Proof Object: Zero-knowledge proof of claim validity</li> <li>Verifier Result: Verification outcome without revealing sensitive details</li> </ul> <p>This process ensures that every piece of extracted data becomes a verifiable, auditable claim that can be trusted across the organization.</p>"},{"location":"architecture/#importance-of-trustless-verification","title":"Importance of Trustless Verification","text":"<p>The claims and attestation process is critical because it moves us toward a trustless, verifiable system where no single document serves as the ultimate source of truth. Instead:</p> <ul> <li>Multiple Sources: Claims are validated against multiple document types and cross-referenced for consistency</li> <li>Cryptographic Verification: Zero-knowledge proofs ensure claim validity without revealing sensitive information</li> <li>Immutable Audit Trail: All claims are stored immutably with complete provenance tracking</li> <li>Discrepancy Detection: The system automatically identifies conflicts between different documents or sources</li> <li>Human-in-the-Loop: When conflicts arise, the system escalates to human experts for resolution</li> <li>Version Control: All document versions and claim revisions are tracked and auditable</li> </ul> <p>This approach eliminates the risk of relying on potentially outdated, incorrect, or manipulated documents as the sole source of truth, creating a robust, verifiable foundation for business decisions.</p>"},{"location":"architecture/#knowledge-management-retrieval","title":"Knowledge Management &amp; Retrieval","text":"<p>Technical Contract-Based Approach: - Versioned Schema Indexing: Documents indexed based on versioned schemas rather than hardcoded categories - Cross-Customer Learning: Insights from one customer's contracts improve others' processing - Adaptive Retrieval: Retrieval strategies adjust based on versioned contract requirements - Cost-Benefit: Managed solutions reduce operational overhead while maintaining cross-industry performance</p>"},{"location":"architecture/#visual-architecture","title":"Visual Architecture","text":"<pre><code>graph TB\n    subgraph \"Technical Contract Layer\"\n        A[Prompt Version&lt;br/&gt;from DB] --&gt; B[Input Schema&lt;br/&gt;Version from DB]\n        A --&gt; C[Model Config&lt;br/&gt;Version from DB]\n        B --&gt; D[Output Schema&lt;br/&gt;Version from DB]\n    end\n\n    subgraph \"Platform Layer\"\n        E[Contract Interpreter] --&gt; F[Universal Agent Framework]\n        F --&gt; G[Supervisor Agent]\n        F --&gt; H[Specialized Agents]\n        F --&gt; I[Claims Processor]\n        F --&gt; J[HITL Agent]\n    end\n\n    subgraph \"Infrastructure Layer\"\n        K[Vertex AI] --&gt; L[ADK Runtime]\n        L --&gt; M[Document Processing]\n        L --&gt; N[Memory Management]\n        L --&gt; O[Evaluation Engine]\n    end\n\n    subgraph \"Immutable Output Layer\"\n        P[AI-Generated Artifacts&lt;br/&gt;Append-Only Log]\n        Q[Claims Log&lt;br/&gt;Append-Only]\n        R[Model Responses&lt;br/&gt;Append-Only]\n    end\n\n    subgraph \"Mutable State Layer\"\n        S[Sitemap&lt;br/&gt;Aggregate State]\n        T[CAD Files&lt;br/&gt;Locked When Generated]\n        U[Site Validation&lt;br/&gt;Mutable Until Locked]\n    end\n\n    subgraph \"External Tools\"\n        V[JIRA Integration]\n        W[Slack Notifications]\n        X[Email Alerts]\n    end\n\n    A --&gt; E\n    E --&gt; F\n    F --&gt; K\n    O --&gt; J\n    P --&gt; S\n    Q --&gt; S\n    R --&gt; S\n    J --&gt; V\n    J --&gt; W\n    J --&gt; X</code></pre>"},{"location":"business-case/","title":"Business Case","text":""},{"location":"business-case/#business-impact","title":"Business Impact","text":""},{"location":"business-case/#current-challenges","title":"Current Challenges","text":"<ul> <li>Engineering time consumed by customer-specific customization</li> <li>Maintenance overhead of hard-coded parsers</li> <li>Manual verification of unreliable outputs</li> <li>Compliance risk from unverifiable processes</li> </ul>"},{"location":"business-case/#measurable-outcomes-with-technical-contract-based-platform","title":"Measurable Outcomes with Technical Contract-Based Platform","text":"<p>By implementing this system, we gain the ability to track meaningful KPIs and OKRs:</p> <ul> <li>Processing Efficiency: Track document processing time and throughput across customers</li> <li>Accuracy Metrics: Monitor confidence scores and verification requirements for outputs</li> <li>Compliance Tracking: Measure adherence to contract requirements and audit readiness</li> <li>Customer Onboarding Speed: Track time from contract signing to productive deployment</li> <li>Resource Utilization: Monitor infrastructure usage and cost optimization</li> <li>Quality Assurance: Measure error rates and refinement cycles</li> <li>Scalability Metrics: Track performance across increasing customer count and document types</li> <li>Time-to-Value: Monitor how quickly new document types and business rules can be implemented</li> </ul>"},{"location":"business-case/#soc2-compliance-security","title":"SOC2 Compliance &amp; Security","text":"<p>The technical contract-based approach provides enhanced SOC2 compliance capabilities:</p> <ul> <li>Auditability: Complete traceability through append-only logs of all AI-generated artifacts</li> <li>Data Integrity: Immutable storage of all claims and model responses</li> <li>Access Controls: Customer-specific role-based access controls</li> <li>Data Encryption: Encryption at rest and in transit for all customer data</li> <li>Change Management: Versioned contracts with immutable references for all changes</li> <li>Monitoring: Real-time performance metrics and anomaly detection</li> </ul>"},{"location":"business-case/#transition-from-automation-to-ai","title":"Transition from Automation to AI","text":"<p>The company began with pure automation using a complex mix of parsers and customer-specific logic. Moving to an AI-driven approach represents a significant shift in methodology, which naturally creates some hesitation among team members.</p> <p>However, AI implementation doesn't have to be magical or unpredictable. By following rigorous patterns and technical contracts, we can create a deterministic and reliable system:</p> <ul> <li>Structured Approach: Technical contracts (prompts, schemas, model configs) provide deterministic frameworks for AI behavior</li> <li>Version Control: All contracts are versioned and immutable, ensuring reproducible results</li> <li>Verification: All AI outputs are treated as \"claims\" with confidence scores and provenance tracking</li> <li>Governance: Artifact governance ensures outputs are composable, auditable, and trustworthy</li> <li>Consistency: The same contract will produce consistent results across time and teams</li> </ul> <p>This approach maintains the predictability that the team values while unlocking the flexibility and intelligence that AI provides. Rather than abandoning deterministic principles, we're applying them to a more sophisticated and capable system.</p>"},{"location":"business-case/#infrastructure-strategy","title":"Infrastructure Strategy","text":"<p>The current web infrastructure operates on AWS, which serves our general computing needs well. However, for AI-specific workloads, GCP offers unparalleled capabilities that AWS simply doesn't match with comparable offerings.</p> <p>This proposal doesn't require moving all infrastructure to GCP. Instead, the AI infrastructure can exist solely in GCP as a specialized service layer that integrates with our existing AWS systems. This approach allows us to:</p> <ul> <li>Leverage GCP's superior AI services without disrupting existing AWS infrastructure</li> <li>Maintain our current AWS investments while gaining access to cutting-edge AI capabilities</li> <li>Create a hybrid architecture that uses each platform for its strengths</li> </ul> <p>Moving other systems to GCP isn't trivial but isn't particularly difficult if that decision is made in the future. Since this is a greenfield deployment for the AI platform, there's no legacy infrastructure to migrate, making GCP the ideal choice for this specialized workload.</p>"},{"location":"business-case/#competitive-analysis-cloud-ai-platforms","title":"Competitive Analysis: Cloud AI Platforms","text":""},{"location":"business-case/#google-cloud-adk-with-vertex-ai-vs-alternatives","title":"Google Cloud ADK with Vertex AI vs. Alternatives","text":""},{"location":"business-case/#google-cloud-adk-with-vertex-ai","title":"Google Cloud ADK with Vertex AI","text":"<ul> <li>Pricing Transparency: Clear, predictable pricing with detailed cost breakdowns available at Vertex AI Pricing</li> <li>Token Economics: Competitive rates with Gemini Flash at $0.075/1M input tokens, significantly lower than competitors</li> <li>Agent-Specific Features: Purpose-built for multi-agent architectures with built-in coordination</li> <li>Horizontal Scalability: Designed for multi-tenant, cross-industry applications</li> <li>Advanced Memory Management: Sophisticated session management that prevents context window overload through intelligent state handling</li> <li>Multimodal Support: Native support for text, image, and document processing in single API calls</li> <li>Enterprise Integration: Seamless integration with multiple enterprise systems</li> </ul>"},{"location":"business-case/#aws-bedrock","title":"AWS Bedrock","text":"<ul> <li>Model Selection: Access to Anthropic Claude, Meta Llama, and Titan models</li> <li>Pricing: Generally higher token costs than Vertex AI, especially for input tokens</li> <li>Infrastructure Integration: Strong AWS ecosystem integration but requires more custom glue code</li> <li>Limitations: Less sophisticated multi-agent coordination and horizontal scaling</li> </ul>"},{"location":"business-case/#azure-openai","title":"Azure OpenAI","text":"<ul> <li>Enterprise Familiarity: Familiar Microsoft ecosystem for many enterprises</li> <li>Model Access: Primarily OpenAI GPT models with some open-source options</li> <li>Pricing: Typically highest among the three platforms</li> <li>Integration: Good for existing Microsoft environments but less flexible for multi-industry approaches</li> </ul>"},{"location":"business-case/#key-differentiators-for-adk-with-vertex-ai","title":"Key Differentiators for ADK with Vertex AI","text":"<ol> <li>Cost Efficiency: Lower token rates across the board, especially for input tokens</li> <li>Agent-Native Architecture: Specifically designed for multi-agent workflows</li> <li>Horizontal Scalability: Built for cross-industry applications from the ground up</li> <li>Advanced Context Management: Superior handling of conversation history and context windows</li> <li>Native Multimodal Processing: Built-in document and image understanding without preprocessing</li> </ol>"},{"location":"business-case/#competitive-advantages","title":"Competitive Advantages","text":""},{"location":"business-case/#why-technical-contract-based-platform-beats-custom-solutions","title":"Why Technical Contract-Based Platform Beats Custom Solutions","text":"<ol> <li>Preserve Excellence: Maintain proven telecom capabilities while enabling scalability</li> <li>Focus on Platform Excellence: Engineers work on platform capabilities, not customer-specific code</li> <li>Enterprise Reliability: Google's infrastructure SLAs and security certifications</li> <li>Cost Efficiency: Pay-per-use eliminates infrastructure waste</li> <li>Rapid Customer Onboarding: New customers defined by versioned technical contracts, not custom development</li> <li>Cross-Industry Learning: Insights from one vertical improve others</li> <li>Unicorn Potential: Horizontal scalability across multiple high-value industries</li> </ol>"},{"location":"business-case/#risk-mitigation","title":"Risk Mitigation","text":""},{"location":"business-case/#addressing-common-concerns","title":"Addressing Common Concerns","text":"<p>Transition Risk: Carefully migrate telecom excellence to technical contract-based system without disrupting current operations.</p> <p>Vendor Lock-in: The ADK approach actually reduces lock-in by providing standardized APIs and protocols that make migration easier than custom-built systems.</p> <p>Performance: Managed services are optimized by Google's infrastructure experts and continuously improved without additional engineering effort.</p> <p>Customization: The technical contract-based system provides flexibility while managing infrastructure concerns.</p> <p>Security: Enterprise-grade security with VPC peering, encryption, and identity management that exceeds what most organizations can implement independently.</p>"},{"location":"business-case/#path-to-unicorn-status","title":"Path to Unicorn Status","text":""},{"location":"business-case/#the-platform-strategy","title":"The Platform Strategy","text":"<ol> <li>Preserve Telecom Excellence: Maintain current capabilities while transitioning to scalable architecture</li> <li>Expand Horizontally: Leverage platform capabilities to enter adjacent industries</li> <li>Network Effects: Accumulate knowledge and capabilities across industries</li> <li>Scale Efficiently: Customer onboarding becomes a technical contract process, not development</li> </ol>"},{"location":"business-case/#conclusion","title":"Conclusion","text":"<p>Moving to a technical contract-based, horizontally scalable platform using ADK with Vertex AI allows Inorsa to preserve its excellent telecom achievements while transitioning away from hard-coded logic that cannot scale. This approach addresses the core industry challenge identified by Zain: \"We don't know how to make model outputs meaningfully composable, auditable, and trustworthy across time, teams, and domains.\"</p> <p>Rather than focusing on static model sequencing (Model A \u2192 Model B \u2192 Model C), our solution emphasizes governance of AI artifacts through technical contracts that ensure composability, auditability, and trustworthiness. The CS-Engineering collaboration for defining technical contracts is critical to success, ensuring that versioned technical implementations perfectly align with customer business requirements.</p> <p>The integrated Human-in-the-Loop (HITL) agents seamlessly handle claim divergences and complex scenarios requiring human judgment, while external tool integrations (JIRA, Slack, etc.) provide comprehensive workflow management and communication capabilities.</p> <p>The combination of Google's robust infrastructure with our technical contract-based agent system creates an unstoppable force for horizontal expansion across industries, positioning Inorsa for unicorn status through scalable, defensible technology. The time to act is now: build the platform that scales horizontally while preserving the telecom excellence that got us here.</p> <p>By focusing on artifact governance rather than model sequencing, we transform AI outputs from unreliable sources to dependable, verifiable business process guides, creating a system that is both technically superior and commercially viable across multiple industries.</p>"},{"location":"contracts/","title":"Technical Contract System","text":""},{"location":"contracts/#the-governance-approach","title":"The Governance Approach","text":"<p>Rather than focusing on static model sequencing (Model A \u2192 Model B \u2192 Model C), we implement technical contracts that govern AI artifacts to ensure they are composable, auditable, and trustworthy across time, teams, and domains.</p>"},{"location":"contracts/#technical-contract-components","title":"Technical Contract Components:","text":"<ol> <li>Prompt Versions: Versioned prompts stored in database with immutable references</li> <li>Input Schema Versions: Versioned input format definitions stored in database</li> <li>Output Schema Versions: Versioned output format definitions stored in database</li> <li>Model Configurations: Versioned model parameters and settings</li> <li>Append-Only Logs: All AI-generated artifacts stored immutably for traceability</li> </ol>"},{"location":"contracts/#artifact-governance-through-technical-contracts","title":"Artifact Governance Through Technical Contracts:","text":"<ul> <li>Versioned Prompts: Database-stored prompts with version control ensure consistency across time and teams</li> <li>Schema Validation: Inputs and outputs validated against versioned schemas for composability</li> <li>Immutable Artifacts: All AI-generated content stored immutably for audit trails across domains</li> <li>Traceability: Complete lineage from input to output through versioned contracts for trustworthiness</li> </ul>"},{"location":"contracts/#cs-engineering-collaboration","title":"CS-Engineering Collaboration","text":"<p>Customer Success (CS) and Engineering collaboration is essential for defining the technical contracts (prompts, schemas, model configs) that govern AI artifacts to ensure they are composable, auditable, and trustworthy across time, teams, and domains.</p>"},{"location":"contracts/#technical-contract-definition-process","title":"Technical Contract Definition Process","text":""},{"location":"contracts/#cs-contributions","title":"CS Contributions:","text":"<ul> <li>Source Input Analysis: Identify and categorize customer document types and formats</li> <li>Business Requirements Mapping: Translate customer business needs into technical specifications</li> <li>Output Validation Criteria: Define what constitutes acceptable output for customer workflows</li> <li>Quality Standards Definition: Establish accuracy thresholds and compliance requirements</li> </ul>"},{"location":"contracts/#engineering-contributions","title":"Engineering Contributions:","text":"<ul> <li>Technical Feasibility Assessment: Evaluate the technical possibility of extracting required data</li> <li>Schema Design: Create formal schemas that map to customer requirements</li> <li>Prompt Engineering: Craft versioned prompts that guide LLMs to desired outputs</li> <li>Model Configuration: Configure model parameters for optimal performance</li> </ul>"},{"location":"contracts/#mutability-matrix","title":"Mutability Matrix","text":"Component Mutability Purpose Traceability Prompt Versions Append-Only Historical record of all prompt iterations Immutable log for debugging Input Schemas Append-Only Versioned input format definitions Immutable for consistency Output Schemas Append-Only Versioned output format definitions Immutable for validation Model Configurations Append-Only Historical model parameters Immutable for reproducibility AI-Generated Artifacts Append-Only All outputs from AI agents Immutable for audit trails Claims Log Append-Only Verified facts from document analysis Immutable for compliance Sitemap Mutable Aggregate of various inputs Mutable for updates until locked CAD Files Mutable \u2192 Locked Design files that become immutable when finalized Mutable until generation complete Site Validation Mutable \u2192 Locked Validation status that locks when complete Mutable until final approval"},{"location":"hitl/","title":"Human-in-the-Loop (HITL) Integration","text":""},{"location":"hitl/#the-critical-role-of-hitl-agents","title":"The Critical Role of HITL Agents","text":"<p>Human-in-the-Loop (HITL) agents are essential for handling complex scenarios where AI agents may diverge or require human judgment. Rather than replacing human expertise, HITL agents enhance the system by seamlessly integrating human oversight into the automated workflow.</p>"},{"location":"hitl/#hitl-integration-process","title":"HITL Integration Process","text":""},{"location":"hitl/#seamless-integration","title":"Seamless Integration:","text":"<ul> <li>Claim Divergence Detection: Automated systems detect when agent claims conflict or fall below confidence thresholds</li> <li>Human Intervention Triggers: Predefined conditions automatically route tasks to human experts for resolution</li> <li>Subject Matter Expert Review: Domain experts review conflicting claims and provide resolution guidance</li> <li>Workflow Continuation: Human decisions are treated as high-confidence claims that continue the automated workflow</li> </ul>"},{"location":"hitl/#resolution-process","title":"Resolution Process:","text":"<ol> <li>Conflict Detection: Claims processor identifies conflicting claims from different agents</li> <li>Confidence Threshold Check: Claims falling below predefined confidence levels trigger HITL</li> <li>Expert Assignment: Appropriate subject matter expert is notified of the conflict</li> <li>Resolution Input: Human expert reviews conflicting claims and provides resolution</li> <li>Claim Integration: Human decision is recorded as high-confidence claim</li> <li>Workflow Resumption: Automated processing continues with human input</li> </ol>"},{"location":"hitl/#subject-matter-expert-involvement","title":"Subject Matter Expert Involvement","text":"<p>Rather than having dedicated HITL agents, the system relies on subject matter experts who help with evaluations and provide resolution when the system and experts align on output scoring and confidence. This approach reduces the need for constant human intervention as the system learns from expert feedback.</p> <p>Experts engage with the system by: - Evaluating outputs and providing feedback on scoring and confidence - Responding to agent requests for resolution when automated processes reach their limits - Training the system through their evaluations to improve future automated decisions - Providing domain knowledge to refine the technical contracts and validation rules</p>"},{"location":"hitl/#handling-divergence-of-claims","title":"Handling Divergence of Claims","text":""},{"location":"hitl/#automatic-detection","title":"Automatic Detection:","text":"<ul> <li>Cross-Agent Validation: Different agents validate each other's claims</li> <li>Confidence Scoring: Claims with low confidence scores are flagged</li> <li>Contradiction Checking: System identifies contradictory claims between agents</li> <li>Pattern Recognition: Machine learning identifies patterns that require human intervention</li> </ul>"},{"location":"hitl/#resolution-workflow","title":"Resolution Workflow:","text":"<ul> <li>Priority Assignment: Conflicts are prioritized based on business impact</li> <li>Expert Routing: Tasks routed to domain experts based on claim type</li> <li>Side-by-Side Comparison: Human experts see conflicting claims with source evidence</li> <li>Decision Recording: Human resolutions are stored as authoritative claims</li> <li>Learning Integration: Human decisions improve future automated processing</li> </ul>"},{"location":"hitl/#external-tool-integration","title":"External Tool Integration","text":""},{"location":"hitl/#jira-integration","title":"JIRA Integration:","text":"<ul> <li>Ticket Creation: HITL tasks automatically create JIRA tickets with full context</li> <li>Status Synchronization: JIRA ticket status updates reflect in the platform</li> <li>Comment Integration: JIRA comments are captured as additional context</li> <li>Assignment Mapping: JIRA assignees correspond to platform human experts</li> </ul>"},{"location":"hitl/#slack-integration","title":"Slack Integration:","text":"<ul> <li>Real-Time Notifications: Urgent HITL tasks trigger Slack notifications</li> <li>Direct Task Assignment: Slack commands can assign or escalate tasks</li> <li>Status Updates: Platform status changes posted to relevant Slack channels</li> <li>Quick Resolution: Simple tasks resolved directly in Slack interface</li> </ul>"},{"location":"hitl/#email-integration","title":"Email Integration:","text":"<ul> <li>Automated Alerts: Email notifications for high-priority HITL tasks</li> <li>Context Provision: Emails include full document context and claim details</li> <li>Response Capture: Email replies are processed as human input</li> <li>Escalation Chains: Unresolved emails automatically escalate to supervisors</li> </ul>"},{"location":"hitl/#other-tool-integrations","title":"Other Tool Integrations:","text":"<ul> <li>CRM Systems: Customer context integrated into HITL workflows</li> <li>Project Management: Task alignment with broader project timelines</li> <li>Communication Platforms: Teams, Discord, or other communication tools</li> <li>Document Management: SharePoint, Confluence, or other document systems</li> </ul>"},{"location":"implementation/","title":"Implementation Roadmap","text":""},{"location":"implementation/#launch-service-with-pure-agentic-flows","title":"Launch Service with Pure Agentic Flows","text":"<ul> <li>Build service with 0 customer-specific code</li> <li>Implement flags and special cases handling through pure agentic flows</li> <li>Develop end-to-end pipeline from raw files to CAD-ready drawings</li> <li>Focus on ground-up development using ADK agent patterns</li> <li>No timeline commitments for additional phases since this is ground-up development</li> </ul>"},{"location":"implementation/#real-world-example-multi-document-site-validation-with-agent-orchestration","title":"Real-World Example: Multi-Document Site Validation with Agent Orchestration","text":"<p>To illustrate how our technical contract-based approach works in practice, here's a real-world example of processing multiple document types for a telecom site validation using agent orchestration with parallel and looper processes:</p>"},{"location":"implementation/#simple-sequential-flow-traditional-approach","title":"Simple Sequential Flow (Traditional Approach):","text":"<pre><code>graph TD\n    A[PDF Ingestion] --&gt; B[Document Parser Tool]\n    B --&gt; C[Structure Normalizer Agent]\n    C --&gt; D[RF Extraction Agent]\n    D --&gt; E[Unit Normalization Agent]\n    E --&gt; F[Validation / Evaluator Agent]\n    F --&gt; G[Knowledge / Claims Storage]</code></pre>"},{"location":"implementation/#parallel-processing-for-multiple-document-types","title":"Parallel Processing for Multiple Document Types:","text":"<pre><code>graph LR\n    subgraph \"Supervisor Agent\"\n        S[Orchestrator]\n    end\n\n    subgraph \"Document Processing Agents\"\n        RFDS[RFDS Extraction&lt;br/&gt;Agent]\n        LEASE[Lease Agreement&lt;br/&gt;Agent]\n        MOUNT[Mount Layout&lt;br/&gt;Agent]\n        EMAIL[Email Analysis&lt;br/&gt;Agent]\n    end\n\n    subgraph \"Shared Resources\"\n        PARSER[Document Parser]\n        VALIDATOR[Validation Agent]\n        STORAGE[Claims Storage]\n    end\n\n    S --&gt; PARSER\n    PARSER --&gt; RFDS\n    PARSER --&gt; LEASE\n    PARSER --&gt; MOUNT\n    PARSER --&gt; EMAIL\n\n    RFDS --&gt; VALIDATOR\n    LEASE --&gt; VALIDATOR\n    MOUNT --&gt; VALIDATOR\n    EMAIL --&gt; VALIDATOR\n\n    VALIDATOR --&gt; STORAGE</code></pre>"},{"location":"implementation/#looper-process-for-validation-and-refinement","title":"Looper Process for Validation and Refinement:","text":"<pre><code>graph TD\n    A[Initial Validation] --&gt; B{Validation&lt;br/&gt;Passes?}\n    B --&gt;|Yes| C[Store Valid Claims]\n    B --&gt;|No| D[Identify Issues]\n    D --&gt; E[Send Feedback to&lt;br/&gt;Extraction Agents]\n    E --&gt; F[Refine Extractions]\n    F --&gt; A</code></pre>"},{"location":"implementation/#writer-refiner-pattern-looper-example","title":"Writer-Refiner Pattern (Looper Example):","text":"<pre><code>graph TD\n    A[Writer Agent&lt;br/&gt;Generates Initial Output] --&gt; B{Quality&lt;br/&gt;Check}\n    B --&gt;|Acceptable| C[Approve Output]\n    B --&gt;|Needs Improvement| D[Refiner Agent&lt;br/&gt;Provides Feedback]\n    D --&gt; E[Writer Agent&lt;br/&gt;Revises Output]\n    E --&gt; B</code></pre> <p>The writer-refiner pattern is a prime example of how looper agents work in practice. In our document processing context, this could manifest as:</p> <ol> <li>Writer Agent: An extraction agent that generates initial interpretations of document sections</li> <li>Quality Check: Validation against technical contracts and schemas</li> <li>Refiner Agent: Another agent that reviews the output and provides specific feedback</li> <li>Revision Loop: The writer agent revises its output based on feedback until quality standards are met</li> </ol> <p>This pattern ensures that outputs meet the required quality standards before being accepted, creating a robust and reliable processing pipeline.</p>"},{"location":"implementation/#code-implementation-example","title":"Code Implementation Example","text":"<p>The ADK framework provides specific patterns for implementing these agent workflows. For our document processing use case, we could implement the writer-refiner pattern as follows:</p>"},{"location":"implementation/#basic-structure","title":"Basic Structure","text":"<pre><code>from vertexai import agent_builder\nfrom vertexai.agent_builder import utils\n\n# Repository Layer for Technical Contracts\nclass ContractRepository:\n    \"\"\"Repository for managing versioned technical contracts\"\"\"\n\n    def get_current_prompt(self, contract_type: str, version: str = None):\n        \"\"\"Get current prompt from repository\"\"\"\n        if version:\n            return self._fetch_prompt(contract_type, version)\n        else:\n            # Get latest version\n            latest_version = self._get_latest_version(contract_type, \"prompt\")\n            return self._fetch_prompt(contract_type, latest_version)\n\n    def get_current_schema(self, schema_type: str, version: str = None):\n        \"\"\"Get current schema from repository\"\"\"\n        if version:\n            return self._fetch_schema(schema_type, version)\n        else:\n            # Get latest version\n            latest_version = self._get_latest_version(schema_type, \"schema\")\n            return self._fetch_schema(schema_type, latest_version)\n\n# Define the writer agent that performs initial extraction\ndef rfds_extraction_agent(document_file):\n    \"\"\"Extracts RF parameters from document files using repository layer\"\"\"\n    # Uses versioned prompts and schemas from technical contracts repository\n    contract_repo = ContractRepository()\n    prompt = contract_repo.get_current_prompt(\"rfds_extraction\", \"2.4\")\n    schema = contract_repo.get_current_schema(\"rf_output\", \"1.0\")\n\n    # Use Vertex AI Search to retrieve relevant sections\n    search_results = vertex_ai_search(document_file.id, query=prompt.context_query)\n\n    result = llm_call(prompt.content, search_results.relevant_sections)\n    return validate_against_schema(result, schema)\n\n# Define the refiner agent that validates and improves outputs\ndef validation_refiner_agent(extracted_data):\n    \"\"\"Validates extraction and provides refinement feedback\"\"\"\n    contract_repo = ContractRepository()\n    schema = contract_repo.get_current_schema(\"rf_output\", \"1.0\")\n    validation_result = validate_against_schema(extracted_data, schema)\n\n    if validation_result.confidence_score &lt; 0.8:\n        feedback = generate_refinement_feedback(extracted_data, validation_result.errors)\n        return {\"needs_revision\": True, \"feedback\": feedback}\n    else:\n        return {\"needs_revision\": False, \"validated_data\": extracted_data}\n\n# Looper implementation\ndef extraction_looper(document_file):\n    \"\"\"Main extraction loop with validation and refinement\"\"\"\n    # Vertex AI Search handles document retrieval and relevant section identification\n    results = []\n\n    # Writer step: initial extraction using file metadata and Vertex AI Search\n    extracted = rfds_extraction_agent(document_file)\n\n    # Looper: validate and refine until quality is met\n    while True:\n        refinement_result = validation_refiner_agent(extracted)\n\n        if not refinement_result[\"needs_revision\"]:\n            results.append(refinement_result[\"validated_data\"])\n            break\n        else:\n            # Apply feedback to improve extraction\n            extracted = apply_feedback(extracted, refinement_result[\"feedback\"])\n\n    return results\n</code></pre>"},{"location":"implementation/#parallel-processing-implementation","title":"Parallel Processing Implementation","text":"<pre><code>import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nasync def process_documents_parallel(document_files):\n    \"\"\"Process multiple document files in parallel using metadata IDs\"\"\"\n    with ThreadPoolExecutor(max_workers=4) as executor:\n        futures = []\n\n        for doc_file in document_files:\n            if doc_file.type == \"rfds\":\n                future = executor.submit(rfds_extraction_agent, doc_file)\n            elif doc_file.type == \"lease\":\n                future = executor.submit(lease_extraction_agent, doc_file)\n            elif doc_file.type == \"mount_layout\":\n                future = executor.submit(mount_extraction_agent, doc_file)\n            elif doc_file.type == \"email\":\n                future = executor.submit(email_analysis_agent, doc_file)\n\n            futures.append(future)\n\n        results = [future.result() for future in futures]\n\n    return results\n</code></pre>"},{"location":"implementation/#integration-with-technical-contracts-and-vertex-ai-search","title":"Integration with Technical Contracts and Vertex AI Search","text":"<pre><code>def rfds_extraction_with_contracts_and_vertex_search(document_file):\n    \"\"\"Extraction using technical contracts with Vertex AI Search integration\"\"\"\n    contract_repo = ContractRepository()\n\n    # Get versioned prompt from repository\n    prompt = contract_repo.get_current_prompt(\"rfds_extraction\", \"2.4\")\n\n    # Use Vertex AI Search to retrieve only relevant document sections\n    # based on the extraction requirements in the prompt\n    search_config = {\n        \"query\": prompt.extraction_requirements,\n        \"filters\": {\"document_id\": document_file.id},\n        \"max_chunks\": 5\n    }\n    search_results = vertex_ai_search.search(search_config)\n\n    # Process only the most relevant sections\n    result = llm_call(prompt.content, search_results.chunks)\n\n    # Validate against versioned schema from repository\n    schema = contract_repo.get_current_schema(\"rf_output\", \"1.0\")\n    validation = validate_against_schema(result, schema)\n\n    return validation\n</code></pre> <p>This code structure demonstrates how the ADK patterns can be applied to our specific document processing use case, with GCS RAG handling document chunking automatically, and clear separation of concerns between extraction, validation, and refinement processes, all governed by versioned technical contracts.</p>"},{"location":"implementation/#agent-orchestration-process","title":"Agent Orchestration Process:","text":"<ol> <li>Customer Initiation: Customer uploads multiple documents (RFDS, Lease Agreement, Mount Layout, related emails) via SharePoint integration</li> <li>Document Classification: System identifies document types and routes appropriately</li> <li>Parallel Processing: Supervisor Agent initiates specialized extraction agents based on document type:</li> <li>RFDS Extraction Agent processes primary radio frequency study</li> <li>Lease Agreement Agent processes legal constraints and permissions</li> <li>Mount Layout Agent processes structural specifications</li> <li>Email Analysis Agent processes related communications for context</li> <li>Cross-Document Validation: Validation Agent checks for consistency across all document types</li> <li>Conflict Resolution: If conflicts arise (e.g., height restrictions in lease vs RFDS), system handles through HITL or refinement</li> <li>Claims Storage: Results stored as immutable claims with full provenance tracking</li> </ol>"},{"location":"implementation/#technical-contract-elements-in-action","title":"Technical Contract Elements in Action:","text":"<ul> <li>Prompt Versions: Each agent receives document-type-specific, versioned prompts (e.g., RFDS Extraction uses prompt v2.4)</li> <li>Input Schema Version: Defines expected structure for each document type (input schema v1.2)</li> <li>Output Schema Version: Dictates final integrated output format (output schema v2.1)</li> <li>Model Configuration: Sets parameters for each agent's operation based on document type</li> <li>Append-Only Log: Records every interaction for auditability and traceability</li> </ul> <p>This example demonstrates how the Supervisor Agent intelligently orchestrates specialized subagents for different document types using parallel processing and looper refinement cycles, while technical contracts ensure deterministic, verifiable processing across all document types.</p>"},{"location":"implementation/#evaluation-framework-synthetic-data-generation","title":"Evaluation Framework &amp; Synthetic Data Generation","text":"<p>Recent developments from OpenAI, Anthropic, and Google have emphasized the critical importance of evaluations in shaping AI for specific business cases. AI models may seem vague and shaky initially, but that's the point - frontier AI requires lab work to properly evaluate and tune models for specific use cases.</p>"},{"location":"implementation/#synthetic-data-generation","title":"Synthetic Data Generation","text":"<p>Google has recently introduced synthetic data generation capabilities within the Vertex stack. Using templates, we can generate thousands or even millions of sample customer documents and run them through our pipeline for comprehensive testing. This approach allows us to:</p> <ul> <li>Test our system against diverse document types and edge cases</li> <li>Validate the accuracy of our technical contracts</li> <li>Identify potential issues before processing real customer data</li> <li>Build comprehensive evaluation datasets for continuous improvement</li> </ul>"},{"location":"implementation/#evaluation-best-practices","title":"Evaluation Best Practices","text":"<ul> <li>Template-Based Testing: Create document templates that represent common customer scenarios</li> <li>Synthetic Data Generation: Use Vertex AI's capabilities to generate realistic test data</li> <li>Continuous Evaluation: Implement ongoing evaluation pipelines to monitor performance</li> <li>Cross-Customer Benchmarking: Compare performance across different customer verticals</li> <li>Golden Dataset Creation: Maintain high-quality sample documents with known outputs for regression testing</li> </ul>"},{"location":"implementation/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"implementation/#technical-contract-management","title":"Technical Contract Management","text":"<ul> <li>Version Control: Implement robust versioning for all technical contracts (prompts, schemas, model configs)</li> <li>Deployment Pipeline: Create automated deployment processes for contract updates</li> <li>Rollback Capability: Ensure quick rollback options for contract changes</li> <li>Testing Framework: Establish comprehensive testing for each contract version</li> </ul>"},{"location":"implementation/#agent-deployment","title":"Agent Deployment","text":"<ul> <li>Containerization: Package agents in containers for consistent deployment</li> <li>Auto-scaling: Implement auto-scaling based on workload demands</li> <li>Monitoring: Set up comprehensive monitoring and alerting for agent performance</li> <li>Load Balancing: Distribute workloads efficiently across agent instances</li> </ul>"},{"location":"implementation/#integration-points","title":"Integration Points","text":"<ul> <li>Document Ingestion: Implement connectors for SharePoint, S3, and other document sources</li> <li>Output Delivery: Create standardized interfaces for delivering results to customer systems</li> <li>Notification Systems: Integrate with Slack, JIRA, and other communication tools</li> <li>Security Protocols: Ensure all integrations meet enterprise security standards</li> </ul>"},{"location":"implementation/#technical-architecture-solutions","title":"Technical Architecture &amp; Solutions","text":""},{"location":"implementation/#1-infrastructure-management-with-adk","title":"1. Infrastructure Management with ADK","text":"<p>Current Issue: Building and maintaining memory layers, chat history, and state management from scratch - Requires custom database schemas for conversation history - Complex locking mechanisms for concurrent access - Serialization/deserialization overhead - Custom retry and error handling logic</p> <p>Solution with ADK: - Managed session state with ADK's built-in session management - Automatic conversation history tracking for each agent - Built-in state persistence and recovery - Native support for long-term memory without custom implementation</p>"},{"location":"implementation/#2-observability-monitoring","title":"2. Observability &amp; Monitoring","text":"<p>Universal Monitoring System: - Integrated tracing and logging for all agent interactions across all customers - Real-time performance metrics per versioned technical contract - Cost tracking and optimization recommendations by customer - Cross-customer performance degradation detection</p>"},{"location":"implementation/#3-evaluation-framework","title":"3. Evaluation Framework","text":"<p>Technical Contract-Based Testing: - Customer-specific evaluation metrics based on versioned contract requirements - Cross-customer benchmarking to identify improvement opportunities - Synthetic data generation based on versioned contract patterns - Continuous evaluation pipelines for each versioned technical contract type</p>"},{"location":"implementation/#4-deployment-cicd","title":"4. Deployment &amp; CI/CD","text":"<p>Customer-Agnostic Deployment: - One-click deployment of versioned technical contract-based agents - Automatic traffic splitting for A/B testing between contract versions - Rollback capabilities with versioned contract management - Integration with existing CI/CD pipelines for platform updates</p>"},{"location":"implementation/#5-multi-tenancy-architecture","title":"5. Multi-Tenancy Architecture","text":"<p>Customer Isolation: - Contract-specific resource allocation - Data isolation with encryption at rest/in transit - Customer-specific role-based access controls - Usage metering per customer contract</p>"},{"location":"implementation/#6-burst-capacity-management","title":"6. Burst Capacity Management","text":"<p>Universal Auto-scaling: - Instant scaling from 0 to thousands of concurrent contract processes - Pay-per-use pricing model per customer contract execution - Regional redundancy for high availability across all customers - Predictive scaling based on cross-customer workload patterns</p>"},{"location":"overview/","title":"Overview of AI Infrastructure Platform","text":""},{"location":"overview/#the-core-challenge","title":"The Core Challenge","text":"<p>The industry problem is not \"We don't know how to sequence models deterministically\" - that's trivial. The real challenge is \"We don't know how to make model outputs meaningfully composable, auditable, and trustworthy across time, teams, and domains.\"</p> <p>Zain's insight is crucial: Static pipelines miss the point because they confuse execution order with system correctness. Agentic frameworks exist because the hard part isn't running models - it's governing the artifacts they produce.</p>"},{"location":"overview/#our-solution","title":"Our Solution","text":"<p>We propose migrating to Google's Agent Development Kit (ADK) with Vertex AI to implement a horizontally scalable, customer-agnostic platform that governs AI artifacts through technical contracts (prompts, schemas, model configs) rather than static pipelines.</p> <p>This approach addresses the core industry challenge by focusing on artifact governance rather than model sequencing, ensuring outputs are composable, auditable, and trustworthy across time, teams, and domains.</p>"},{"location":"overview/#key-differences-libraries-vs-platforms","title":"Key Differences: Libraries vs Platforms","text":"<p>Working with stateless third-party libraries presents unique challenges that highlight the distinction between AI libraries and managed platforms:</p> <ul> <li>Stateless Third-Party Libraries: Offer short-term convenience at long-term expense; require building your own infrastructure for state management, persistence, observability, and governance. Useful for testing and prototyping but not for enterprise solutions.</li> <li>AI Platforms (like ADK): Provide managed services for state, tracing, memory, and orchestration as part of the platform</li> </ul>"},{"location":"overview/#challenges-with-current-approaches","title":"Challenges with Current Approaches","text":"<p>Using stateless libraries combined with high-cost vision LLMs is not an optimal combination for document processing:</p> <ol> <li>Document Processing Complexity: Stateless libraries often require running LLMs repeatedly over entire documents (like detailed RFDS files) rather than first extracting core data with specialized parsers</li> <li>State Management: Stateless approaches can struggle with state persistence and recovery in complex workflows</li> <li>Resource Optimization: Running expensive vision LLMs over entire documents multiple times instead of parsing structured data first can be inefficient</li> <li>Scalability: Stateless architectures may face challenges when scaling across multiple customers and document types</li> </ol>"},{"location":"overview/#adk-advantages","title":"ADK Advantages","text":"<p>The ADK documentation provides detailed information about how the platform addresses these challenges:</p> <ul> <li>Sessions documentation explains how ADK manages conversation state and persistence</li> <li>Context management details how ADK handles memory and context windows efficiently</li> </ul> <p>ADK's approach allows us to: - Parse structured data first with specialized tools - Pass only relevant, extracted information to LLMs - Maintain proper state management without custom infrastructure - Scale efficiently across multiple customers and document types</p>"},{"location":"overview/#key-benefits","title":"Key Benefits","text":"<ul> <li>Preserve Excellence: Maintain proven telecom capabilities while enabling scalability</li> <li>Focus on Platform Excellence: Engineers work on platform capabilities, not customer-specific code</li> <li>Cost Efficiency: Pay-per-use eliminates infrastructure waste</li> <li>Rapid Customer Onboarding: New customers defined by versioned technical contracts, not custom development</li> <li>Cross-Industry Learning: Insights from one vertical improve others</li> <li>Unicorn Potential: Horizontal scalability across multiple high-value industries</li> </ul>"}]}